{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JauD1Z0rHYE8"
      },
      "outputs": [],
      "source": [
        "# !touch bert_dataset.py\n",
        "# !touch bert_trainer.py\n",
        "# !touch trainer.py\n",
        "# !touch finetune_plm_native.py\n",
        "# !touch classify_plm.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bert_dataset"
      ],
      "metadata": {
        "id": "1Xcl-Mw06qsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextClassificationCollator():\n",
        "    def __init__(self, tokenizer, max_length, with_text=True):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.with_text = with_text\n",
        "\n",
        "    # 매번 데이터로더가 미니배치 사이즈가 128이다. 그러면 128개의 데이터셋에 대해서 getitem 호출한 것을 받아왔다.\n",
        "    # 받아온것을 concat하면 된다. / 그것을 지금 못하니 call_fn을 부른다.\n",
        "    # samples에 데이터셋이 리턴한게 리스트로 들어있을 것이다.\n",
        "    # 즉, 딕셔너리에 리스트가 들어있을 것이다.\n",
        "    def __call__(self, samples):\n",
        "        texts = [s['text'] for s in samples]\n",
        "        labels = [s['label'] for s in samples]\n",
        "        \n",
        "        # 토크나이저를 사용한다.\n",
        "        # __call__이 호출된다. \n",
        "        # 토큰갯수 기준으로 미니배치 사이즈는 가변적 대신 미니배치내의 토큰갯수만 바뀜 그러면 메모리는 고정\n",
        "        # -> 구현 어려움 그래서 미니배치네 가장 긴 기준으로\n",
        "        encoding = self.tokenizer(\n",
        "            texts, # text\n",
        "            padding=True, # 미니배치네 가장 긴 기준으로 패딩을 하기위해 max_length를 getitem에서 안쓰는 거다.\n",
        "            truncation=True, # max_length 기준으로 잘라냄\n",
        "            return_tensors=\"pt\", # pytorch type으로 \n",
        "            max_length=self.max_length \n",
        "        )\n",
        "\n",
        "        return_value = {\n",
        "            'input_ids' : encoding['input_ids'], # (x,l,1) -> 샘플, 타임스켑, 인덱스\n",
        "            'attention_mask' : encoding['attention_mask'], # padding된 부분 학습하지 않기 위함\n",
        "            'labels' : torch.tensor(labels, dtype=torch.long), # 리스트로 있던것을 torch.long 타입의 텐서로 바꿈          \n",
        "        }\n",
        "        if self.with_text: # 위 텍스트가 true인 경우에는 return_value['text]에 넣는다.\n",
        "            return_value['text'] = texts\n",
        "\n",
        "        return return_value\n",
        "\n",
        "\n",
        "class TextClassificationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, texts, labels): # 전체 데이터셋(코퍼스), 각 샘플별 레이블을 리스트로 들고옴\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self): # 전체 샘플이 몇개인지\n",
        "        return len(self.texts)\n",
        "\n",
        "    # 데이터셋을 데이터 데이터 로더에 넣을 건데 필요할때마다 미니배치를만들어서 메 iteration 리턴을 한다.\n",
        "    # 미니배치가 128이면 128개의의 데이터셋에 대해서 getitem을 호출한다.\n",
        "    # 매번 호출할때마다 idx에 있는 아이템들을 리턴해주면 된다.\n",
        "    # 문장의 길이가 다 다를것이기 때문에 미니 배치내에 가장 긴 문장을 기준으로 패딩을 채워서 리턴한다.\n",
        "    def __getitem__(self, item): \n",
        "        text = str(self.texts[item])\n",
        "        label = self.labels[item]\n",
        "\n",
        "        return { # 길이가 모두 같다면 딕셔너리를 안쓰고 그냥 tensor로 리턴하면된다.\n",
        "            'text' : text,\n",
        "            'label' : label,\n",
        "        }"
      ],
      "metadata": {
        "id": "zCb1v7xwfJjS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Typical Model Training Procedure\n",
        "- **Epoch 시작**\n",
        "  - Training\n",
        "     - **Iteration 시작** : 미니배치마다 돈다.\n",
        "     - Feed-forward : 미니배치를 모델에 통과\n",
        "     - Loss 계산 : 모델로 y의 헷을 얻게 된다./거기에 y의 확률값 likeihood를 구하여 loss를 계산\n",
        "     - Back-propagation : loss에 대해서 파라미터로 미분해주는 역전파를 수행/각 layer에 weight parameter에 .grad라는 곳애 채워줌\n",
        "     - Gradient Descent 수행 : 그것을 활용해서 optimizer에서 optimizer.step식으로 함수를 호출하게 되면 optimizer가 자기가 담당하고 있는 파라미터들을 gradient를 보고 gradient Descent를 1step 수행해주게 된다.\n",
        "     - 현재 상태 출력 : 현재 loss, gradient 크기, accuracy등을 출력\n",
        "     - **Iteration 종료** : 한 iteration을 종료하고 다음 iteration을 한다. 모든 미니배치가 끝나면 1epoch가 끝나게 된다.\n",
        "  - Validation\n",
        "    - **Iteration 시작** : 1epoch가 끝나면 시작\n",
        "    - Feed-forward : 모델에 통과\n",
        "    - Loss 계산 : Loss를 계산하고 역전파, gradent를 할필요 없음음\n",
        "    - 현재 상태 출력 : \n",
        "    - **Iteration 종료**\n",
        "  - 현재 epoch에서 validation loss가 이전보다 낮아졌는지 오버피팅을 체크\n",
        "  - 모델이 저장되어야 함\n",
        "- **Epoch 종료**\n",
        "- 사용자가 지정한 epoch만큼 반복"
      ],
      "metadata": {
        "id": "OqdKe02Rv3L3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer"
      ],
      "metadata": {
        "id": "CSqW4rHfILLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ignite\n",
        "!pip install pytorch-ignite\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvP-tnKm65yt",
        "outputId": "3c363e53-42da-4cc2-d417-b0cc940a95e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ignite\n",
            "  Downloading ignite-1.1.0-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.9/dist-packages (from ignite) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from ignite) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->ignite) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->ignite) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->ignite) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->ignite) (3.4)\n",
            "Installing collected packages: ignite\n",
            "Successfully installed ignite-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-ignite\n",
            "  Downloading pytorch_ignite-0.4.11-py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.5/266.5 KB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from pytorch-ignite) (23.0)\n",
            "Requirement already satisfied: torch<3,>=1.3 in /usr/local/lib/python3.9/dist-packages (from pytorch-ignite) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<3,>=1.3->pytorch-ignite) (4.5.0)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "from ignite.engine import Engine\n",
        "from ignite.engine import Events\n",
        "from ignite.metrics import RunningAverage\n",
        "from ignite.contrib.handlers.tqdm_logger import ProgressBar\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/인공지능/텍스트분류')\n",
        "from simple_ntc.utils import get_grad_norm, get_parameter_norm\n",
        "\n",
        "VERBOSE_SILENT = 0\n",
        "VERBOSE_EPOCH_WISE = 1\n",
        "VERBOSE_BATCH_WISE = 2\n",
        "\n",
        "\n",
        "class MyEngine(Engine):\n",
        "\n",
        "    def __init__(self, func, model, crit, optimizer, config):\n",
        "        # Ignite Engine does not have objects in below lines.\n",
        "        # Thus, we assign class variables to access these object, during the procedure.\n",
        "        self.model = model\n",
        "        self.crit = crit\n",
        "        self.optimizer = optimizer\n",
        "        self.config = config\n",
        "\n",
        "        super().__init__(func) # Ignite Engine only needs function to run.\n",
        "\n",
        "        self.best_loss = np.inf\n",
        "        self.best_model = None\n",
        "\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "    @staticmethod\n",
        "    def train(engine, mini_batch):\n",
        "        # You have to reset the gradients of all model parameters\n",
        "        # before to take another step in gradient descent.\n",
        "        engine.model.train() # Because we assign model as class variable, we can easily access to it.\n",
        "        engine.optimizer.zero_grad()\n",
        "\n",
        "        x, y = mini_batch.text, mini_batch.label\n",
        "        x, y = x.to(engine.device), y.to(engine.device)\n",
        "\n",
        "        x = x[:, :engine.config.max_length]\n",
        "\n",
        "        # Take feed-forward\n",
        "        y_hat = engine.model(x)\n",
        "\n",
        "        loss = engine.crit(y_hat, y)\n",
        "        loss.backward()\n",
        "\n",
        "        # Calculate accuracy only if 'y' is LongTensor,\n",
        "        # which means that 'y' is one-hot representation.\n",
        "        if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.LongTensor):\n",
        "            accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() / float(y.size(0))\n",
        "        else:\n",
        "            accuracy = 0\n",
        "\n",
        "        p_norm = float(get_parameter_norm(engine.model.parameters()))\n",
        "        g_norm = float(get_grad_norm(engine.model.parameters()))\n",
        "\n",
        "        # Take a step of gradient descent.\n",
        "        engine.optimizer.step()\n",
        "\n",
        "        return {\n",
        "            'loss': float(loss),\n",
        "            'accuracy': float(accuracy),\n",
        "            '|param|': p_norm,\n",
        "            '|g_param|': g_norm,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def validate(engine, mini_batch):\n",
        "        engine.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x, y = mini_batch.text, mini_batch.label\n",
        "            x, y = x.to(engine.device), y.to(engine.device)\n",
        "\n",
        "            x = x[:, :engine.config.max_length]\n",
        "\n",
        "            y_hat = engine.model(x)\n",
        "\n",
        "            loss = engine.crit(y_hat, y)\n",
        "\n",
        "            if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.LongTensor):\n",
        "                accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() / float(y.size(0))\n",
        "            else:\n",
        "                accuracy = 0\n",
        "\n",
        "        return {\n",
        "            'loss': float(loss),\n",
        "            'accuracy': float(accuracy),\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def attach(train_engine, validation_engine, verbose=VERBOSE_BATCH_WISE):\n",
        "        # Attaching would be repaeted for serveral metrics.\n",
        "        # Thus, we can reduce the repeated codes by using this function.\n",
        "        def attach_running_average(engine, metric_name):\n",
        "            RunningAverage(output_transform=lambda x: x[metric_name]).attach( # engine에 해당 metric_name으로 attach한다.\n",
        "                engine,\n",
        "                metric_name,\n",
        "            )\n",
        "\n",
        "        training_metric_names = ['loss', 'accuracy', '|param|', '|g_param|']\n",
        "\n",
        "        for metric_name in training_metric_names:\n",
        "            attach_running_average(train_engine, metric_name)\n",
        "\n",
        "        # If the verbosity is set, progress bar would be shown for mini-batch iterations.\n",
        "        # Without ignite, you can use tqdm to implement progress bar.\n",
        "        if verbose >= VERBOSE_BATCH_WISE:\n",
        "            pbar = ProgressBar(bar_format=None, ncols=120) # 프로그래스바 attach\n",
        "            pbar.attach(train_engine, training_metric_names)\n",
        "\n",
        "        # If the verbosity is set, statistics would be shown after each epoch.\n",
        "        # train engine에 epoch가 끝났을때 train_engin에 등록해서 print_train_logs()를 실행해서 해당하는 최종값을 받아와라라\n",
        "        if verbose >= VERBOSE_EPOCH_WISE: \n",
        "            @train_engine.on(Events.EPOCH_COMPLETED)\n",
        "            def print_train_logs(engine):\n",
        "                print('Epoch {} - |param|={:.2e} |g_param|={:.2e} loss={:.4e} accuracy={:.4f}'.format(\n",
        "                    engine.state.epoch,\n",
        "                    engine.state.metrics['|param|'],\n",
        "                    engine.state.metrics['|g_param|'],\n",
        "                    engine.state.metrics['loss'],\n",
        "                    engine.state.metrics['accuracy'],\n",
        "                ))\n",
        "\n",
        "        # validation도 마찬가지로\n",
        "        validation_metric_names = ['loss', 'accuracy']\n",
        "        \n",
        "        for metric_name in validation_metric_names:\n",
        "            attach_running_average(validation_engine, metric_name)\n",
        "\n",
        "        # Do same things for validation engine.\n",
        "        if verbose >= VERBOSE_BATCH_WISE:\n",
        "            pbar = ProgressBar(bar_format=None, ncols=120)\n",
        "            pbar.attach(validation_engine, validation_metric_names)\n",
        "\n",
        "        if verbose >= VERBOSE_EPOCH_WISE:\n",
        "            @validation_engine.on(Events.EPOCH_COMPLETED)\n",
        "            def print_valid_logs(engine):\n",
        "                print('Validation - loss={:.4e} accuracy={:.4f} best_loss={:.4e}'.format(\n",
        "                    engine.state.metrics['loss'],\n",
        "                    engine.state.metrics['accuracy'],\n",
        "                    engine.best_loss,\n",
        "                ))\n",
        "\n",
        "    @staticmethod\n",
        "    def check_best(engine):\n",
        "        loss = float(engine.state.metrics['loss'])\n",
        "        if loss <= engine.best_loss: # If current epoch returns lower validation loss,\n",
        "            engine.best_loss = loss  # Update lowest validation loss.\n",
        "            engine.best_model = deepcopy(engine.model.state_dict()) # Update best model weights.\n",
        "\n",
        "    @staticmethod\n",
        "    def save_model(engine, train_engine, config, **kwargs):\n",
        "        torch.save(\n",
        "            {\n",
        "                'model': engine.best_model,\n",
        "                'config': config,\n",
        "                **kwargs\n",
        "            }, config.model_fn\n",
        "        )\n",
        "\n",
        "\n",
        "class Trainer():\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        model, crit, optimizer,\n",
        "        train_loader, valid_loader,\n",
        "    ):\n",
        "        train_engine = MyEngine(\n",
        "            MyEngine.train,\n",
        "            model, crit, optimizer, self.config\n",
        "        )\n",
        "        validation_engine = MyEngine(\n",
        "            MyEngine.validate,\n",
        "            model, crit, optimizer, self.config\n",
        "        )\n",
        "\n",
        "        MyEngine.attach(\n",
        "            train_engine,\n",
        "            validation_engine,\n",
        "            verbose=self.config.verbose\n",
        "        )\n",
        "\n",
        "        def run_validation(engine, validation_engine, valid_loader):\n",
        "            validation_engine.run(valid_loader, max_epochs=1)\n",
        "\n",
        "        train_engine.add_event_handler(\n",
        "            Events.EPOCH_COMPLETED, # event\n",
        "            run_validation, # function\n",
        "            validation_engine, valid_loader, # arguments\n",
        "        )\n",
        "        validation_engine.add_event_handler(\n",
        "            Events.EPOCH_COMPLETED, # event\n",
        "            MyEngine.check_best, # function\n",
        "        )\n",
        "\n",
        "        train_engine.run(\n",
        "            train_loader,\n",
        "            max_epochs=self.config.n_epochs,\n",
        "        )\n",
        "\n",
        "        model.load_state_dict(validation_engine.best_model)\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "vhJ94s_LIMnq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bert_trainer"
      ],
      "metadata": {
        "id": "RS2gMz4q6tFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.utils as torch_utils\n",
        "\n",
        "from ignite.engine import Events\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/인공지능/텍스트분류')\n",
        "from simple_ntc.utils import get_grad_norm, get_parameter_norm\n",
        "\n",
        "VERBOSE_SILENT = 0\n",
        "VERBOSE_EPOCH_WISE = 1\n",
        "VERBOSE_BATCH_WISE = 2\n",
        "\n",
        "from simple_ntc.trainer import Trainer, MyEngine\n",
        "\n",
        "\n",
        "class EngineForBert(MyEngine):\n",
        "\n",
        "    def __init__(self, func, model, crit, optimizer, scheduler, config):\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "        super().__init__(func, model, crit, optimizer, config)\n",
        "\n",
        "    @staticmethod\n",
        "    def train(engine, mini_batch):\n",
        "        # You have to reset the gradients of all model parameters\n",
        "        # before to take another step in gradient descent.\n",
        "        engine.model.train() # Because we assign model as class variable, we can easily access to it.\n",
        "        engine.optimizer.zero_grad()\n",
        "\n",
        "        x, y = mini_batch['input_ids'], mini_batch['labels']\n",
        "        x, y = x.to(engine.device), y.to(engine.device) # gpu로 옮김김\n",
        "        mask = mini_batch['attention_mask']\n",
        "        mask = mask.to(engine.device) # gpu로 옮김김\n",
        "\n",
        "        x = x[:, :engine.config.max_length] # n.l,1 : ㅣ차원에 대해서 잘라서 슬라이싱 한다.\n",
        "\n",
        "        # Take feed-forward\n",
        "        y_hat = engine.model(x, attention_mask=mask).logits # .logits==hidden state==softmax 넣기 직전값, linear layer통과해 차원축소함함\n",
        "        # y_hat : (n,|c|)\n",
        "\n",
        "        loss = engine.crit(y_hat, y) #crossentropy를 통과시키면 loss가 나온다.\n",
        "        loss.backward() # loss를 미분해서 역전파함\n",
        "\n",
        "        # Calculate accuracy only if 'y' is LongTensor,\n",
        "        # which means that 'y' is one-hot representation.\n",
        "        if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.LongTensor):\n",
        "            accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() / float(y.size(0))\n",
        "        else:\n",
        "            accuracy = 0\n",
        "\n",
        "        p_norm = float(get_parameter_norm(engine.model.parameters())) # parameter의 L2_norm\n",
        "        g_norm = float(get_grad_norm(engine.model.parameters())) # gradient의 L2_norm\n",
        "\n",
        "        # Take a step of gradient descent.\n",
        "        engine.optimizer.step() # step을 먹여준다.gradient desent하여 한 스텝을 파라미터에 업데이트\n",
        "        engine.scheduler.step()\n",
        "\n",
        "        return {\n",
        "            'loss': float(loss),\n",
        "            'accuracy': float(accuracy),\n",
        "            '|param|': p_norm,\n",
        "            '|g_param|': g_norm,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def validate(engine, mini_batch):\n",
        "        engine.model.eval()\n",
        "\n",
        "        with torch.no_grad(): # grad계산할 필요가 없음/ 메모리를 작게 빠르게게\n",
        "            x, y = mini_batch['input_ids'], mini_batch['labels']\n",
        "            x, y = x.to(engine.device), y.to(engine.device)\n",
        "            mask = mini_batch['attention_mask']\n",
        "            mask = mask.to(engine.device)\n",
        "\n",
        "            x = x[:, :engine.config.max_length]\n",
        "\n",
        "            # Take feed-forward\n",
        "            y_hat = engine.model(x, attention_mask=mask).logits\n",
        "\n",
        "            loss = engine.crit(y_hat, y)\n",
        "\n",
        "            if isinstance(y, torch.LongTensor) or isinstance(y, torch.cuda.LongTensor):\n",
        "                accuracy = (torch.argmax(y_hat, dim=-1) == y).sum() / float(y.size(0))\n",
        "            else:\n",
        "                accuracy = 0\n",
        "\n",
        "        return {\n",
        "            'loss': float(loss),\n",
        "            'accuracy': float(accuracy),\n",
        "        }\n",
        "\n",
        "\n",
        "class BertTrainer(Trainer):\n",
        "\n",
        "    def __init__(self, config): # 학습을 위한 하이퍼파라미터라 들어있는 config를 가져옴\n",
        "        self.config = config\n",
        "\n",
        "    def train( # 학습할때 모델, loss함수, optimizer...를 받아온다.\n",
        "        self,\n",
        "        model, crit, optimizer, scheduler,\n",
        "        train_loader, valid_loader,\n",
        "    ):\n",
        "        train_engine = EngineForBert(\n",
        "            EngineForBert.train,\n",
        "            model, crit, optimizer, scheduler, self.config\n",
        "        )\n",
        "        validation_engine = EngineForBert(\n",
        "            EngineForBert.validate,\n",
        "            model, crit, optimizer, scheduler, self.config\n",
        "        )\n",
        "\n",
        "        # trainer.py에 선언되어있음/ 현재상태 출력을 위한 것을 등록\n",
        "        # train_engine과 validation_engine의 현재 상태를 출력력\n",
        "        EngineForBert.attach( \n",
        "            train_engine,\n",
        "            validation_engine,\n",
        "            verbose=self.config.verbose\n",
        "        )\n",
        "\n",
        "        # 학습이 끝나고 validation을 실행하도록 \n",
        "        # 실행하는 함수를 만들고 train에 등록록\n",
        "        def run_validation(engine, validation_engine, valid_loader):\n",
        "            validation_engine.run(valid_loader, max_epochs=1)\n",
        "\n",
        "        train_engine.add_event_handler(\n",
        "            Events.EPOCH_COMPLETED, # event\n",
        "            run_validation, # function\n",
        "            validation_engine, valid_loader, # arguments\n",
        "        )\n",
        "        # best loss 여부체크 및 모델 저장장\n",
        "        validation_engine.add_event_handler(\n",
        "            Events.EPOCH_COMPLETED, # event\n",
        "            EngineForBert.check_best, # function\n",
        "        )\n",
        "\n",
        "        # train engine 실행 train_loader를 넣고 몇 epoch를 돌릴것인지 지정\n",
        "        train_engine.run(\n",
        "            train_loader,\n",
        "            max_epochs=self.config.n_epochs,\n",
        "        )\n",
        "\n",
        "        # 다 끝나면 베스트 모델을 불러온다음에 return하면 학습이 종료된다.\n",
        "        model.load_state_dict(validation_engine.best_model)\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "KLidHThPzAaR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## finetune_plm_native.py"
      ],
      "metadata": {
        "id": "Z5wb1Y8nLUQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch_optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moylA6FWLpVC",
        "outputId": "7cf0c250-be31-49bc-d82e-38b0d84e53c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from torch_optimizer) (1.13.1+cu116)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.5.0->torch_optimizer) (4.5.0)\n",
            "Installing collected packages: pytorch-ranger, torch_optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import argparse\n",
        "# import random\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# from transformers import BertTokenizerFast\n",
        "# from transformers import BertForSequenceClassification, AlbertForSequenceClassification\n",
        "# from transformers import AdamW\n",
        "# from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# import torch_optimizer as custom_optim\n",
        "\n",
        "# import sys \n",
        "# sys.path.append('/content/drive/MyDrive/인공지능/텍스트분류')\n",
        "# from simple_ntc.bert_trainer import BertTrainer as Trainer\n",
        "# from simple_ntc.bert_dataset import TextClassificationDataset, TextClassificationCollator\n",
        "# from simple_ntc.utils import read_text\n",
        "\n",
        "# def define_argparser():\n",
        "#     p = argparse.ArgumentParser()\n",
        "\n",
        "#     p.add_argument('--model_fn', required=True)\n",
        "#     p.add_argument('--train_fn', required=True)\n",
        "#     # Recommended model list:\n",
        "#     # - kykim/bert-kor-base\n",
        "#     # - kykim/albert-kor-base\n",
        "#     # - beomi/kcbert-base\n",
        "#     # - beomi/kcbert-large\n",
        "#     p.add_argument('--pretrained_model_name', type=str, default='beomi/kcbert-base')\n",
        "#     p.add_argument('--use_albert', action='store_true')\n",
        "    \n",
        "#     p.add_argument('--gpu_id', type=int, default=-1)\n",
        "#     p.add_argument('--verbose', type=int, default=2)\n",
        "\n",
        "#     p.add_argument('--batch_size', type=int, default=32)\n",
        "#     p.add_argument('--n_epochs', type=int, default=5)\n",
        "\n",
        "#     p.add_argument('--lr', type=float, default=5e-5) # warmup이 끝났을때 lr이다.\n",
        "#     p.add_argument('--warmup_ratio', type=float, default=.2) # 트랜스포머가 학습이 까다로움/그냥 adam쓰면 성능이 잘 안나옴 \n",
        "#     p.add_argument('--adam_epsilon', type=float, default=1e-8)\n",
        "#     # If you want to use RAdam, I recommend to use LR=1e-4.\n",
        "#     # Also, you can set warmup_ratio=0.\n",
        "#     p.add_argument('--use_radam', action='store_true') # warmup안하고 하는 방법 연구 이것을 쓸대의 인자는 바로 위에 2개임임\n",
        "#     p.add_argument('--valid_ratio', type=float, default=.2)\n",
        "\n",
        "#     p.add_argument('--max_length', type=int, default=100)\n",
        "\n",
        "#     config = p.parse_args()\n",
        "\n",
        "#     return config\n",
        "\n",
        "\n",
        "# def get_loaders(fn, tokenizer, valid_ratio=.2):\n",
        "#     # Get list of labels and list of texts.\n",
        "#     labels, texts = read_text(fn)\n",
        "\n",
        "#     # Generate label to index map.\n",
        "#     unique_labels = list(set(labels)) # 유니크한 레이블로 만든다.\n",
        "#     label_to_index = {}\n",
        "#     index_to_label = {}\n",
        "#     for i, label in enumerate(unique_labels): # 유니크레이블을 돌면서 매핑\n",
        "#         label_to_index[label] = i\n",
        "#         index_to_label[i] = label\n",
        "\n",
        "#     # Convert label text to integer value.\n",
        "#     # 텍스트를 index로 변환해 나온 결과를 적용하면 interger의 리스트가 된다.\n",
        "#     labels = list(map(label_to_index.get, labels))\n",
        "\n",
        "#     # Shuffle before split into train and validation set.\n",
        "#     # shuffle을 해서 train과 vali를 나눈다.\n",
        "#     shuffled = list(zip(texts, labels)) # zip해논 상태에서 shuffled 해야한다.\n",
        "#     random.shuffle(shuffled)\n",
        "#     texts = [e[0] for e in shuffled]\n",
        "#     labels = [e[1] for e in shuffled]\n",
        "#     idx = int(len(texts) * (1 - valid_ratio))\n",
        "\n",
        "#     # Get dataloaders using given tokenizer as collate_fn.\n",
        "#     # 데이터로더가 나온다. train이니가 shuffle해야한다. val은 안한다.\n",
        "#     train_loader = DataLoader(\n",
        "#         TextClassificationDataset(texts[:idx], labels[:idx]),\n",
        "#         batch_size=config.batch_size,\n",
        "#         shuffle=True,\n",
        "#         collate_fn=TextClassificationCollator(tokenizer, config.max_length),\n",
        "#     )\n",
        "#     valid_loader = DataLoader(\n",
        "#         TextClassificationDataset(texts[idx:], labels[idx:]),\n",
        "#         batch_size=config.batch_size,\n",
        "#         collate_fn=TextClassificationCollator(tokenizer, config.max_length),\n",
        "#     )\n",
        "\n",
        "#     return train_loader, valid_loader, index_to_label\n",
        "\n",
        "\n",
        "# def get_optimizer(model, config):\n",
        "#     if config.use_radam:\n",
        "#         optimizer = custom_optim.RAdam(model.parameters(), lr=config.lr)\n",
        "#     else:\n",
        "#         # Prepare optimizer and schedule (linear warmup and decay)\n",
        "#         no_decay = ['bias', 'LayerNorm.weight']\n",
        "#         optimizer_grouped_parameters = [\n",
        "#             {\n",
        "#                 'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "#                 'weight_decay': 0.01\n",
        "#             },\n",
        "#             {\n",
        "#                 'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "#                 'weight_decay': 0.0\n",
        "#             }\n",
        "#         ]\n",
        "\n",
        "#         optimizer = optim.AdamW( # 웬만해서는 default값 사용\n",
        "#             optimizer_grouped_parameters,\n",
        "#             lr=config.lr,\n",
        "#             eps=config.adam_epsilon\n",
        "#         )\n",
        "\n",
        "#     return optimizer\n",
        "\n",
        "\n",
        "# def main(config):\n",
        "#     # Get pretrained tokenizer.\n",
        "#     tokenizer = BertTokenizerFast.from_pretrained(config.pretrained_model_name)\n",
        "#     # Get dataloaders using tokenizer from untokenized corpus.\n",
        "#     train_loader, valid_loader, index_to_label = get_loaders( # idnex_to_label은 추론할때 필요한 정보보\n",
        "#         config.train_fn,\n",
        "#         tokenizer,\n",
        "#         valid_ratio=config.valid_ratio\n",
        "#     )\n",
        "#     # 몇 개인지 확인인\n",
        "#     print(\n",
        "#         '|train| =', len(train_loader) * config.batch_size,\n",
        "#         '|valid| =', len(valid_loader) * config.batch_size,\n",
        "#     )\n",
        "\n",
        "#     # warmup\n",
        "#     # adam은 고정 lr이다. 이렇게 하면 transformer가 학습이 잘안됨\n",
        "#     # 그래서 warmup을 한다. adam이 처음부터 잘동작한다.\n",
        "#     # 그러나 처음 들어오는 샘플들이 noise할 수 있다. 그걸로 모멘텀을 잘 못 배워서 날라가버리는 현상발생\n",
        "#     # 초반에 네트워크가 안정되기 전까지 많이 배우지 말고 warmup을 해라는 것이다.    \n",
        "#     n_total_iterations = len(train_loader) * config.n_epochs # 미니배치수 X epoch수로 iteration을 지정\n",
        "#     n_warmup_steps = int(n_total_iterations * config.warmup_ratio) # 400의 20%면 80까지는 warmup한다.\n",
        "#     print(\n",
        "#         '#total_iters =', n_total_iterations,\n",
        "#         '#warmup_iters =', n_warmup_steps,\n",
        "#     )\n",
        "\n",
        "#     # 모델 선언\n",
        "#     # Get pretrained model with specified softmax layer.\n",
        "#     model_loader = AlbertForSequenceClassification if config.use_albert else BertForSequenceClassification\n",
        "#     model = model_loader.from_pretrained(\n",
        "#         config.pretrained_model_name, # 사전 학습된 weight가 로딩이 됨,\n",
        "#         num_labels=len(index_to_label) # 다만 맨위에 있는 linear layer는 random 초기화되어 있다.\n",
        "#     )\n",
        "#     optimizer = get_optimizer(model, config)\n",
        "\n",
        "#     # By default, model returns a hidden representation before softmax func.\n",
        "#     # Thus, we need to use CrossEntropyLoss, which combines LogSoftmax and NLLLoss.\n",
        "#     # 소프트맥스 직전의 hidden_referengentation...값을 loss에 집어넣으면 된다.\n",
        "#     # 그것을 logits이라고 한다. \n",
        "#     # 그리고 linear 스케줄 warmup\n",
        "#     crit = nn.CrossEntropyLoss()    \n",
        "#     scheduler = get_linear_schedule_with_warmup(\n",
        "#         optimizer,\n",
        "#         n_warmup_steps,\n",
        "#         n_total_iterations\n",
        "#     )\n",
        "\n",
        "#     # gpu로 옮김김\n",
        "#     if config.gpu_id >= 0:\n",
        "#         model.cuda(config.gpu_id)\n",
        "#         crit.cuda(config.gpu_id)\n",
        "\n",
        "#     # Start train.\n",
        "#     trainer = Trainer(config)\n",
        "#     model = trainer.train(\n",
        "#         model,\n",
        "#         crit,\n",
        "#         optimizer,\n",
        "#         scheduler,\n",
        "#         train_loader,\n",
        "#         valid_loader,\n",
        "#     )\n",
        "\n",
        "#     torch.save({\n",
        "#         'rnn': None,\n",
        "#         'cnn': None,\n",
        "#         'bert': model.state_dict(),\n",
        "#         'config': config, # 나중에 불러올 때 어떤 hp인지 알아야함함\n",
        "#         'vocab': None,\n",
        "#         'classes': index_to_label,\n",
        "#         'tokenizer': tokenizer,\n",
        "#     }, config.model_fn)\n",
        "\n",
        "# # 실행을 하면 여기로 간다.\n",
        "# # hyper parameter를 여기에 입력받게 된다.\n",
        "# if __name__ == '__main__':\n",
        "#     config = define_argparser()\n",
        "#     main(config)"
      ],
      "metadata": {
        "id": "ELXWL5Ie63wW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## classify_plm.py"
      ],
      "metadata": {
        "id": "VrlsAVd3ap4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35l1Ucl8pwrd",
        "outputId": "072baa93-03a3-402d-98ad-9640c7865c5c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.9/dist-packages (0.14.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchtext) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtext) (1.22.4)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.9/dist-packages (from torchtext) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtext) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->torchtext) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# import argparse\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from torchtext import data\n",
        "\n",
        "# from transformers import BertTokenizerFast\n",
        "# from transformers import BertForSequenceClassification, AlbertForSequenceClassification\n",
        "\n",
        "\n",
        "# def define_argparser():\n",
        "#     '''\n",
        "#     Define argument parser to take inference using pre-trained model.\n",
        "#     '''\n",
        "#     p = argparse.ArgumentParser()\n",
        "\n",
        "#     p.add_argument('--model_fn', required=True)\n",
        "#     p.add_argument('--gpu_id', type=int, default=-1)\n",
        "#     p.add_argument('--batch_size', type=int, default=256)\n",
        "#     p.add_argument('--top_k', type=int, default=1)\n",
        "\n",
        "#     config = p.parse_args()\n",
        "\n",
        "#     return   \n",
        "\n",
        "\n",
        "# def read_text():\n",
        "#     '''\n",
        "#     Read text from standard input for inference.\n",
        "#     '''\n",
        "#     lines = []\n",
        "\n",
        "#     for line in sys.stdin:\n",
        "#         if line.strip() != '':\n",
        "#             lines += [line.strip()]\n",
        "\n",
        "#     return lines\n",
        "\n",
        "\n",
        "# def main(config):\n",
        "#     saved_data = torch.load( # 저장된 모델을 불러옴\n",
        "#         config.model_fn,\n",
        "#         map_location='cpu' if config.gpu_id < 0 else 'cuda:%d' % config.gpu_id # 원하는 디바이스에 로딩되도록록\n",
        "#     )\n",
        "\n",
        "#     train_config = saved_data['config']\n",
        "#     bert_best = saved_data['bert']\n",
        "#     index_to_label = saved_data['classes']\n",
        "\n",
        "#     lines = read_text()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         # Declare model and load pre-trained weights.\n",
        "#         tokenizer = BertTokenizerFast.from_pretrained(train_config.pretrained_model_name)\n",
        "#         model_loader = AlbertForSequenceClassification if train_config.use_albert else BertForSequenceClassification\n",
        "#         model = model_loader.from_pretrained(\n",
        "#             train_config.pretrained_model_name,\n",
        "#             num_labels=len(index_to_label)\n",
        "#         )\n",
        "#         model.load_state_dict(bert_best) # fine-tuning한 파라미터를 로드한다.\n",
        "\n",
        "#         if config.gpu_id >= 0:\n",
        "#             model.cuda(config.gpu_id)\n",
        "#         device = next(model.parameters()).device # 모델의 첫번째 파라미터의 디바이스를 보면 어느 디바이스에 올랐는지 알 수 있음\n",
        "\n",
        "#         # Don't forget turn-on evaluation mode.\n",
        "#         model.eval()\n",
        "\n",
        "#         y_hats = []\n",
        "#         for idx in range(0, len(lines), config.batch_size): # 전체에 대해 batch_size만큼 점프하면서 인덱스를 맏아온다.\n",
        "#             mini_batch = tokenizer(\n",
        "#                 lines[idx:idx + config.batch_size],#lines에서 indx부터 그 다음 batch_size까지 받아옴\n",
        "#                 padding=True,\n",
        "#                 truncation=True,\n",
        "#                 return_tensors=\"pt\",\n",
        "#             )\n",
        "\n",
        "#             x = mini_batch['input_ids']\n",
        "#             x = x.to(device)\n",
        "#             mask = mini_batch['attention_mask']\n",
        "#             mask = mask.to(device)\n",
        "\n",
        "#             # Take feed-forward\n",
        "#             # model(x, attention_mask=mask) : (n,1,|c|) or (n,|c|)\n",
        "#             #  F.softmax 확률값 구하기 위함\n",
        "#             # dim = -1를 해야지 |c|에 대해서 softmax를 구한다.\n",
        "#             # 같은 크기지만 각 미니배치별 샘플별 클래스가 들어잇는 확률을 구하게 된다.\n",
        "#             y_hat = F.softmax(model(x, attention_mask=mask).logits, dim=-1) \n",
        "\n",
        "#             # y_hats에 쌓는다.\n",
        "#             y_hats += [y_hat]\n",
        "#         # Concatenate the mini-batch wise result\n",
        "#         # (n,|c|) X mini_batch 갯수\n",
        "#         # 이것을 다 합쳐야 된다.\n",
        "#         y_hats = torch.cat(y_hats, dim=0)\n",
        "#         # |y_hats| = (len(lines), n_classes)\n",
        "\n",
        "#         probs, indice = y_hats.cpu().topk(config.top_k)\n",
        "#         # |indice| = (len(lines), top_k)\n",
        "\n",
        "#         # 화면에 출력\n",
        "#         for i in range(len(lines)):\n",
        "#             sys.stdout.write('%s\\t%s\\n' % (\n",
        "#                 ' '.join([index_to_label[int(indice[i][j])] for j in range(config.top_k)]), \n",
        "#                 lines[i]\n",
        "#             ))\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     config = define_argparser()\n",
        "#     main(config)"
      ],
      "metadata": {
        "id": "F4pfy5p0pp0e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 실행"
      ],
      "metadata": {
        "id": "V8EPNnPet2QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/인공지능/텍스트분류"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6AyQg0puSqg",
        "outputId": "e00f3770-cea3-4688-eb02-a90293225469"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/인공지능/텍스트분류\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python finetune_plm_native.py --model_fn ./models/review.native.kcbert.pth --train_fn ./data/review.sorted.uniq.refined.shuf.train.tsv --gpu_id 0 --batch_size 80 --n_epochs 2 --pretrained_model_name 'beomi/kcbert-base'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p46jeOwuvB5j",
        "outputId": "c8b68272-14f3-45ed-dc9e-a8186360ab2b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-22 05:49:37.814965: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-22 05:49:37.985528: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-03-22 05:49:38.838071: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-22 05:49:38.838183: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-22 05:49:38.838203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "|train| = 192000 |valid| = 48000\n",
            "#total_iters = 4800 #warmup_iters = 960\n",
            "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1 - |param|=2.97e+02 |g_param|=2.03e+00 loss=2.0269e-01 accuracy=0.9282\n",
            "Validation - loss=1.8501e-01 accuracy=0.9353 best_loss=inf\n",
            "Epoch 2 - |param|=2.97e+02 |g_param|=1.81e+00 loss=1.4100e-01 accuracy=0.9516\n",
            "Validation - loss=1.8073e-01 accuracy=0.9398 best_loss=1.8501e-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "28h_fTYHvxyj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}