{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- 자유도는 좀 떨어지지만 구현이 굉장히 간단함"
      ],
      "metadata": {
        "id": "pkUEKqdJiSW_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oot67k7EbVGP"
      },
      "outputs": [],
      "source": [
        "# !touch finetune_plm_hftrainer.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68FdhWwvmMuz",
        "outputId": "6c25fd57-ce5b-43f5-8268-1073e82cd282"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import argparse\n",
        "# import random\n",
        "\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# import torch\n",
        "\n",
        "# from transformers import BertTokenizerFast\n",
        "# from transformers import BertForSequenceClassification, AlbertForSequenceClassification\n",
        "# from transformers import Trainer\n",
        "# from transformers import TrainingArguments\n",
        "\n",
        "# import sys\n",
        "# sys.path.append('/content/drive/MyDrive/인공지능/텍스트분류')\n",
        "# from simple_ntc.bert_dataset import TextClassificationCollator\n",
        "# from simple_ntc.bert_dataset import TextClassificationDataset\n",
        "# from simple_ntc.utils import read_text\n",
        "\n",
        "\n",
        "# def define_argparser():\n",
        "#     p = argparse.ArgumentParser()\n",
        "\n",
        "#     p.add_argument('--model_fn', required=True)\n",
        "#     p.add_argument('--train_fn', required=True)\n",
        "#     # Recommended model list:\n",
        "#     # - kykim/bert-kor-base\n",
        "#     # - kykim/albert-kor-base\n",
        "#     # - beomi/kcbert-base\n",
        "#     # - beomi/kcbert-large\n",
        "#     p.add_argument('--pretrained_model_name', type=str, default='beomi/kcbert-base')\n",
        "#     p.add_argument('--use_albert', action='store_true')\n",
        "\n",
        "#     p.add_argument('--valid_ratio', type=float, default=.2)\n",
        "#     p.add_argument('--batch_size_per_device', type=int, default=32)\n",
        "#     p.add_argument('--n_epochs', type=int, default=5)\n",
        "\n",
        "#     p.add_argument('--warmup_ratio', type=float, default=.2)\n",
        "\n",
        "#     p.add_argument('--max_length', type=int, default=100)\n",
        "\n",
        "#     config = p.parse_args()\n",
        "\n",
        "#     return config\n",
        "\n",
        "\n",
        "# def get_datasets(fn, valid_ratio=.2):\n",
        "#      # Get list of labels and list of texts.\n",
        "#     labels, texts = read_text(fn)\n",
        "\n",
        "#     # Generate label to index map.\n",
        "#     unique_labels = list(set(labels))\n",
        "#     label_to_index = {}\n",
        "#     index_to_label = {}\n",
        "#     for i, label in enumerate(unique_labels):\n",
        "#         label_to_index[label] = i\n",
        "#         index_to_label[i] = label\n",
        "\n",
        "#     # Convert label text to integer value.\n",
        "#     labels = list(map(label_to_index.get, labels))\n",
        "\n",
        "#     # Shuffle before split into train and validation set.\n",
        "#     shuffled = list(zip(texts, labels))\n",
        "#     random.shuffle(shuffled)\n",
        "#     texts = [e[0] for e in shuffled]\n",
        "#     labels = [e[1] for e in shuffled]\n",
        "#     idx = int(len(texts) * (1 - valid_ratio))\n",
        "\n",
        "#     train_dataset = TextClassificationDataset(texts[:idx], labels[:idx])\n",
        "#     valid_dataset = TextClassificationDataset(texts[idx:], labels[idx:])\n",
        "\n",
        "#     return train_dataset, valid_dataset, index_to_label\n",
        "\n",
        "\n",
        "# def main(config):\n",
        "#     # Get pretrained tokenizer.\n",
        "#     tokenizer = BertTokenizerFast.from_pretrained(config.pretrained_model_name)\n",
        "#     # Get datasets and index to label map.\n",
        "#     train_dataset, valid_dataset, index_to_label = get_datasets(\n",
        "#         config.train_fn,\n",
        "#         valid_ratio=config.valid_ratio\n",
        "#     )\n",
        "\n",
        "#     print(\n",
        "#         '|train| =', len(train_dataset),\n",
        "#         '|valid| =', len(valid_dataset),\n",
        "#     )\n",
        "\n",
        "#     total_batch_size = config.batch_size_per_device * torch.cuda.device_count() \n",
        "#     n_total_iterations = int(len(train_dataset) / total_batch_size * config.n_epochs) #샘플수/전체배치사이즈*n_epoch\n",
        "#     n_warmup_steps = int(n_total_iterations * config.warmup_ratio)\n",
        "#     print(\n",
        "#         '#total_iters =', n_total_iterations,\n",
        "#         '#warmup_iters =', n_warmup_steps,\n",
        "#     )\n",
        "\n",
        "#     # Get pretrained model with specified softmax layer.\n",
        "#     model_loader = AlbertForSequenceClassification if config.use_albert else BertForSequenceClassification\n",
        "#     model = model_loader.from_pretrained(\n",
        "#         config.pretrained_model_name,\n",
        "#         num_labels=len(index_to_label)\n",
        "#     )\n",
        "\n",
        "#     training_args = TrainingArguments(\n",
        "#         output_dir='/content/drive/MyDrive/인공지능/텍스트분류/hf_checkpoints',\n",
        "#         num_train_epochs=config.n_epochs,\n",
        "#         per_device_train_batch_size=config.batch_size_per_device,\n",
        "#         per_device_eval_batch_size=config.batch_size_per_device,\n",
        "#         warmup_steps=n_warmup_steps,\n",
        "#         weight_decay=0.01,\n",
        "#         fp16=True,# 기본이 32bit float이다. 16으로 하게되면 메모리도 적게 쓰고 2000번째 gpu부터는 속도가 훨씬 빠르다.\n",
        "#         evaluation_strategy='epoch',\n",
        "#         logging_steps=n_total_iterations // 100,\n",
        "#         save_steps=n_total_iterations // config.n_epochs,\n",
        "#         load_best_model_at_end=True,\n",
        "#     )\n",
        "\n",
        "#     def compute_metrics(pred):\n",
        "#         labels = pred.label_ids\n",
        "#         preds = pred.predictions.argmax(-1) # 가장 확률이 높음 값의 인덱스를 가져와라\n",
        "\n",
        "#         return {\n",
        "#             'accuracy': accuracy_score(labels, preds)\n",
        "#         }\n",
        "\n",
        "#     trainer = Trainer(\n",
        "#         model=model,\n",
        "#         args=training_args,\n",
        "#         data_collator=TextClassificationCollator(tokenizer,\n",
        "#                                        config.max_length,\n",
        "#                                        with_text=False),\n",
        "#         train_dataset=train_dataset, # 기존은 데이터로더를 넘겨줬는데 지금은 데이터로드가 안에 있는 것이다.\n",
        "#         eval_dataset=valid_dataset,\n",
        "#         compute_metrics=compute_metrics,\n",
        "#     )\n",
        "\n",
        "#     trainer.train()\n",
        "\n",
        "#     torch.save({\n",
        "#         'rnn': None,\n",
        "#         'cnn': None,\n",
        "#         'bert': trainer.model.state_dict(),\n",
        "#         'config': config,\n",
        "#         'vocab': None,\n",
        "#         'classes': index_to_label,\n",
        "#         'tokenizer': tokenizer,\n",
        "#     }, config.model_fn)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__': # 진입 지점\n",
        "#     config = define_argparser()\n",
        "#     main(config)"
      ],
      "metadata": {
        "id": "et3UqoGjlWLl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train\n",
        "- 왜인지는 모르겠는제 오류가 생김"
      ],
      "metadata": {
        "id": "hJezq7earHRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/인공지능/텍스트분류/finetune_plm_hftrainer.py --model_fn /content/drive/MyDrive/인공지능/텍스트분류/models/review.hft.bert.pth --train_fn /content/drive/MyDrive/인공지능/텍스트분류/data/review.sorted.uniq.refined.shuf.train.tsv --batch_size_per_device 96 --n_epochs 2"
      ],
      "metadata": {
        "id": "JVlxVFmmrITr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 결과 확인"
      ],
      "metadata": {
        "id": "XcOYL5mL29cD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2u2PGlRrxJB"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}